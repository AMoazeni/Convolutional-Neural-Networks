{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "This article explores a Machine Learning algorithm called Convolution Neural Network (CNN), it's a common Deep Learning technique used for image recognition and classification.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "You are provided with a dataset consisting of 5,000 Cat images and 5,000 Dog images. We are going to train a Machine Learning model to learn differences between the two categories. The model will predict if a new unseen image is a Cat or Dog. The code architecture is robust and can be used to recognize any number of image categories, if provided with enough data.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Convolution Neural Networks (CNN)\n",
    "\n",
    "<br></br>\n",
    "Convolution Neural Networks are good for pattern recognition and feature detection which is especially useful in image classification. Improve the performance of Convolution Neural Networks through hyper-parameter tuning, adding more convolution layers, adding more fully connected layers, or providing more correctly labeled data to the algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Create a Convolution Neural Network (CNN) with the following steps:\n",
    "\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Full Connection\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Check out [How to implement a neural network](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/), also take a look at [A Friendly Introduction to Cross-Entropy Loss](http://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Convolution is a function derived from two other functions through an integration that expresses how the shape of one is modified by the other.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/01%20-%20Convolution%20Equation.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "For image recognition, we convolve the input image with Feature Detectors (also known as Kernel or Filter) to generate a Feature Map (also known as Convolved Map or Activation Map). This reveals and preserves patterns in the image, and also compresses the image for easier processing. Feature Maps are generated by element-wise multiplication and addition of corresponding images with Filters consisting of multiple Feature Detectors. This allows the creation of multiple Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/02%20-%20CNN%20Example.png\" width=\"500\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/03%20-%20CNN%20Feature%20Map.png\" width=\"500\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/04%20-%20CNN%20Multi%20Feature%20Map.png\" width=\"500\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "This [Image Convolution Guide](https://docs.gimp.org/en/plug-in-convmatrix.html) allows you to play with various filters applied to an image. Edge Detect is a useful filters in Machine Learning. The algorithm creates filters that are not recognizable to humans, perhaps we learn with similar techniques in our subconscious. Feature Maps preserve spatial relationships between pixels throughout processing.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/05%20-%20Edge%20Detect%20Filter.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Rectified Linear Units (ReLU)\n",
    "\n",
    "<br></br>\n",
    "Rectifier Functions are applied to Convolution Neural Networks to increases non-linearity (breaks up linearity). This is an important step for image recognition with CNNs. Images are usually non-linear due to sharp transition of pixels, different colors, etc. ReLU functions help amplify the non-linearity of images so the ML model has an easier time finding patterns. \n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/06%20-%20ReLU%20Layer.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "### Before ReLU\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/07%20-%20Before%20ReLU.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "###  After ReLU\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/08%20-%20After%20ReLU.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "In the above example, the ReLU operation removed the Black Pixels so there's less White to Gray to Black transitions. Borders now have more abrupt Pixel changes. Microsoft argues that the using their Modified Rectifier Function works better for CNNs.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/09%20-%20Modified%20Rectifier.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Max Pooling\n",
    "\n",
    "<br></br>\n",
    "Max Pooling finds the largest value of small grids in the Feature Map, this creates a Pooled Feature Map. Average Pooling (sub-sampling) takes the average values of small grids.  It makes sure that your Neural Network has Spatial Invariance (able to find learned features in new images that are slightly varied or distorted). Max Pooling provides resilience against shifter or rotated features. It also further distills Feature Maps (reduces size) while preserving spatial relationships of pixels. Removing unnecessary information also helps prevent overfitting. Read 'Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition.pdf'. Here is an online [CNN Visualization Tool](http://scs.ryerson.ca/~aharley/vis/conv/flat.html).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/10%20-%20Max%20Pooling.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Flattening\n",
    "\n",
    "<br></br>\n",
    "Flattening puts values of the pooled Feature Map matrix into a 1-D vector. This makes it easy for the image data to pass through an Artificial Neural Network algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/11%20-%20Flattening.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/12%20-%20Flattening%202.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Full Connection\n",
    "\n",
    "<br></br>\n",
    "This is when the output of a Convolution Neural Network is flattened and fed through a classic Artificial Neural Network. It's important to note that CNNs require fully-connected hidden layers where as regular ANNs don't necessarily need full connections.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/13%20-%20Full%20Connection.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The process of CNN back-propagation adjusts weights of neurons, while adjusting Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/14%20-%20CNN%20Backprop.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "When it's time for the CNN to make a decision between Cat or Dog, the final layer neurons 'vote' on probability of an image being a Cat or Dog (or any other categories you show it). The Neural Network adjusts votes according to the best weights it has determined through back-propagation.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/15%20-%20CNN%20Weighted%20Votes.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is a summary of every step of a CNN, don't forget about the Rectifier Function that removes linearity in Feature Maps, also remember that the hidden layers are fully connected.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/16%20-%20CNN%20Full.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Pre-Processing (Images Augmentation)\n",
    "\n",
    "<br></br>\n",
    "This step modifies images to prevent over-fitting. This data augmentation trick can generate tons more data by applying random modifications to existing data like shearing, stretching, zooming, etc. This makes your dataset and algorithm more robust and generalized.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Softmax and Cross-Entropy Cost Function\n",
    "\n",
    "<br></br>\n",
    "The Softmax function shown below is used to make sure that the probabilities of the output layer add up to one, this gives us a percentage guess. Watch this Geoffrey Hinton [video about the SoftMax Function](https://www.youtube.com/watch?v=mlaLLQofmR8).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/18%20-%20Softmax%20Function.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "We had previously used the Mean Squared Error (MSE) Cost Function. For CNNs, it's better to use the Cross-Entropy Function as your Cost Function. We use Cross-Entropy as a Loss Function because it has a 'Log' term which helps amplify small Errors and better guide gradient descent.\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/17%20-%20Log%20Loss%20Function.png\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/19%20-%20Cross%20Entropy%20Function.png\" width=\"200\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/20%20-%20Cross%20Entropy%20Plug%20In.png\" width=\"400\">\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Images/21%20-%20Error%20Comparison.png\">\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network\n",
    "# Part 1 - Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "# 'Sequential' library used to Initialize NN as sequence of layers (Alternative to Graph initialization)\n",
    "from keras.models import Sequential\n",
    "# 'Conv2D' for 1st step of adding convolution layers to images ('Conv3D' for videos with time as 3rd dimension)\n",
    "from keras.layers import Conv2D\n",
    "# 'MaxPooling2D' step 2 for pooling of max values from Convolution Layers\n",
    "from keras.layers import MaxPooling2D\n",
    "# 'Flatten' Pooled Layers for step 3\n",
    "from keras.layers import Flatten\n",
    "# 'Dense' for fully connected layers that feed into classic ANN\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "# Calling this object a 'classifier' because that's its job\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "# Apply a method 'add' on the object 'classifier'\n",
    "# Filter = Feature Detector = Feature Kernel\n",
    "# 'Conv2D' (Number of Filters, (Filter Row, Filter Column), input shape of inputs = (3 color channels, 64x64 -> 256x256 dimension of 2D array in each channel))\n",
    "# Start with 32 filters, work your way up to 64 -> 128 -> 256\n",
    "# 'input_shape' needs all picture inputs to be the same shape and format (2D array for B&W, 3D for Color images with each 2D array channel being Blue/Green/Red)\n",
    "# 'input_shape' parameter shape matters (3,64,64) vs (64,64,3)\n",
    "# 'Relu' Rectifier Activation Function used to get rid of -ve pixel values and increase non-linearity\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "# Reduces the size of the Feature Map by half (eg. 5x5 turns into 3x3 or 8x8 turns into 4x4)\n",
    "# Preserves Spatial Structure and performance of model while reducing computation time\n",
    "# 'pool_size' at least needs to be 2x2 to preserve Spatial Structure information (context around individual pixels)\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer to improve performance\n",
    "# Only need 'input_shape' for Input Layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "# Take all the Pooled Feature Maps and put them into one huge single Vector that will input into a classic NN\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "# Add some fully connected hidden layers (start with a number of Node between input and output layers)\n",
    "# [Input Nodes(huge) - Output Nodes (2: Cat or Dog)] / 2 = ~128?...\n",
    "# 'Activation' function makes sure relavent Nodes get a stronger vote or no vote\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "# Add final Output Layer with binary options\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "# 'adam' Stochastic Gradient Descent optimizer\n",
    "# 'loss' function. Logarithmic loss for 2 categories use 'binary_crossentropy' and 'categorical_crossentropy' for more objects\n",
    "# 'metric' is the a performance metric\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create random transformation from Data to increase Dataset and prevent overfitting\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# 'batch_size' is the number of images that go through the CNN every weight update cycle\n",
    "# Increase 'target_size' to improve model accuracy \n",
    "training_set = train_datagen.flow_from_directory('/Users/mac/Google Drive/Python & RasPi/Udemy Deep Learning/Deep Learning Code/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/Users/mac/Google Drive/Python & RasPi/Udemy Deep Learning/Deep Learning Code/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 10975s 1s/step - loss: 0.3722 - acc: 0.8242 - val_loss: 0.5651 - val_acc: 0.8071\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 4466s 558ms/step - loss: 0.1281 - acc: 0.9497 - val_loss: 0.9259 - val_acc: 0.8023\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 4387s 548ms/step - loss: 0.0643 - acc: 0.9764 - val_loss: 1.1272 - val_acc: 0.7995\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 4394s 549ms/step - loss: 0.0458 - acc: 0.9840 - val_loss: 1.1933 - val_acc: 0.7968\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 3785s 473ms/step - loss: 0.0354 - acc: 0.9878 - val_loss: 1.3235 - val_acc: 0.7825\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 2154s 269ms/step - loss: 0.0289 - acc: 0.9903 - val_loss: 1.7144 - val_acc: 0.7619\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 2155s 269ms/step - loss: 0.0251 - acc: 0.9918 - val_loss: 1.4543 - val_acc: 0.7939\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 2123s 265ms/step - loss: 0.0218 - acc: 0.9928 - val_loss: 1.4498 - val_acc: 0.7997\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 2201s 275ms/step - loss: 0.0191 - acc: 0.9938 - val_loss: 1.4667 - val_acc: 0.8041\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 2047s 256ms/step - loss: 0.0171 - acc: 0.9943 - val_loss: 1.6030 - val_acc: 0.7898\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 2030s 254ms/step - loss: 0.0160 - acc: 0.9950 - val_loss: 1.7126 - val_acc: 0.7943\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 2035s 254ms/step - loss: 0.0149 - acc: 0.9951 - val_loss: 1.6413 - val_acc: 0.7954\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 2039s 255ms/step - loss: 0.0137 - acc: 0.9959 - val_loss: 1.6788 - val_acc: 0.8040\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 2038s 255ms/step - loss: 0.0129 - acc: 0.9959 - val_loss: 1.6795 - val_acc: 0.8007\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 2037s 255ms/step - loss: 0.0121 - acc: 0.9959 - val_loss: 1.5820 - val_acc: 0.8114\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 2035s 254ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 1.7047 - val_acc: 0.8009\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 2041s 255ms/step - loss: 0.0111 - acc: 0.9966 - val_loss: 1.6471 - val_acc: 0.7945\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 2044s 256ms/step - loss: 0.0103 - acc: 0.9968 - val_loss: 1.8675 - val_acc: 0.7953\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 2059s 257ms/step - loss: 0.0100 - acc: 0.9971 - val_loss: 1.7338 - val_acc: 0.8028\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 2062s 258ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 1.6616 - val_acc: 0.7906\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 2041s 255ms/step - loss: 0.0093 - acc: 0.9972 - val_loss: 1.8274 - val_acc: 0.7839\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 2031s 254ms/step - loss: 0.0092 - acc: 0.9972 - val_loss: 1.6794 - val_acc: 0.8057\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 2033s 254ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 1.7334 - val_acc: 0.7995\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 2130s 266ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 1.7782 - val_acc: 0.8009\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 2307s 288ms/step - loss: 0.0088 - acc: 0.9975 - val_loss: 1.6895 - val_acc: 0.8044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18162ced30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Increase 'epochs' to boost model performance\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.Adam at 0x18161d9978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to file\n",
    "# Architecture of the model, allowing to re-create the model\n",
    "# Weights of the model\n",
    "# Training configuration (loss, optimizer)\n",
    "# State of the optimizer, allowing to resume training exactly where you left off\n",
    "classifier.save('../CNN_Cat_Dog_Model.h5')\n",
    "\n",
    "# Examine model\n",
    "classifier.summary()\n",
    "\n",
    "# Examine Weights\n",
    "classifier.weights\n",
    "\n",
    "# Examine Optimizer\n",
    "classifier.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved Model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../CNN_Cat_Dog_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model class indices are: {'cats': 0, 'dogs': 1}\n",
      "\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "# Numpy used for preprocessing\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/Users/mac/Google Drive/Python & RasPi/Udemy Deep Learning/Deep Learning Code/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/dataset/single_prediction/cat_or_dog_3.jpg', target_size = (64, 64))\n",
    "# Add a 3rd Color dimention to match Model expectation\n",
    "test_image = image.img_to_array(test_image)\n",
    "# Add one more dimension to beginning of image array so 'Predict' function can recieve it (corresponds to Batch, even if only one batch)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "# We now need to pull up the mapping between 0/1 and cat/dog\n",
    "training_set.class_indices\n",
    "# Map is 2D so check the first row, first column value\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "# Print result\n",
    "\n",
    "print(\"The model class indices are:\", training_set.class_indices)\n",
    "\n",
    "print(\"\\n\" + prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
